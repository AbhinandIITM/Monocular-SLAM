{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0331293",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from PIL import Image\n",
    "from scipy.spatial.transform import Rotation as R \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dfe824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open3D Version: 0.19.0\n",
      "Open3D Path: d:\\Miniconda\\envs\\SLAM\\lib\\site-packages\\open3d\\__init__.py\n",
      "Has Sphere class: False\n",
      "Has create_mesh_sphere function: False\n",
      "Contents of o3d.geometry: ['All', 'Average', 'AxisAlignedBoundingBox', 'Color', 'DeformAsRigidAsPossibleEnergy', 'FilterScope', 'Gaussian3', 'Gaussian5', 'Gaussian7', 'Geometry', 'Geometry2D', 'Geometry3D', 'HalfEdge', 'HalfEdgeTriangleMesh', 'Image', 'ImageFilterType', 'KDTreeFlann', 'KDTreeSearchParam', 'KDTreeSearchParamHybrid', 'KDTreeSearchParamKNN', 'KDTreeSearchParamRadius', 'LineSet', 'MeshBase', 'Normal', 'Octree', 'OctreeColorLeafNode', 'OctreeInternalNode', 'OctreeInternalPointNode', 'OctreeLeafNode', 'OctreeNode', 'OctreeNodeInfo', 'OctreePointColorLeafNode', 'OrientedBoundingBox', 'PointCloud', 'Quadric', 'RGBDImage', 'SimplificationContraction', 'Smoothed', 'Sobel3dx', 'Sobel3dy', 'Spokes', 'TetraMesh', 'TriangleMesh', 'Vertex', 'Voxel', 'VoxelGrid', '__doc__', '__loader__', '__name__', '__package__', '__spec__', 'get_rotation_matrix_from_axis_angle', 'get_rotation_matrix_from_quaternion', 'get_rotation_matrix_from_xyz', 'get_rotation_matrix_from_xzy', 'get_rotation_matrix_from_yxz', 'get_rotation_matrix_from_yzx', 'get_rotation_matrix_from_zxy', 'get_rotation_matrix_from_zyx', 'keypoint', 'm', 'n']\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "print(\"Open3D Version:\", o3d.__version__)\n",
    "print(\"Open3D Path:\", o3d.__file__)\n",
    "print(\"Has Sphere class:\", hasattr(o3d.geometry, 'Sphere'))\n",
    "print(\"Has create_mesh_sphere function:\", hasattr(o3d.geometry, 'create_mesh_sphere'))\n",
    "print(\"Contents of o3d.geometry:\", dir(o3d.geometry))\n",
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e6a9351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dataset path\n",
    "DATASET_PATH = r'rgbd_dataset_freiburg3_long_office_household'  \n",
    "\n",
    "fx = 525.0\n",
    "fy = 525.0\n",
    "cx = 319.5\n",
    "cy = 239.5\n",
    "\n",
    "K = np.array([[fx, 0, cx],\n",
    "              [0, fy, cy],\n",
    "              [0,  0,  1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77f51855",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "class TUMDataset(Dataset):\n",
    "    def __init__(self, rgb_file, depth_file, gt_file, transform=None, depth_transform=None, base_dir=\"\"):\n",
    "        rgb_file = os.path.join(base_dir,rgb_file)\n",
    "        depth_file = os.path.join(base_dir,depth_file)\n",
    "        gt_file = os.path.join(base_dir,gt_file)\n",
    "        self.rgb_data = self._read_file(rgb_file)\n",
    "        self.depth_data = self._read_file(depth_file)\n",
    "        self.gt_data = self._read_file(gt_file)\n",
    "\n",
    "        n = min(len(self.rgb_data), len(self.depth_data), len(self.gt_data))\n",
    "        self.rgb_data = self.rgb_data[:n]\n",
    "        self.depth_data = self.depth_data[:n]\n",
    "        self.gt_data = self.gt_data[:n]\n",
    "\n",
    "        self.transform = transform\n",
    "        self.depth_transform = depth_transform\n",
    "        self.base_dir = base_dir\n",
    "\n",
    "    def _read_file(self, filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = [line.strip() for line in f if line.strip() and not line.startswith(\"#\")]\n",
    "        data = []\n",
    "        for line in lines:\n",
    "            parts = line.replace(\",\", \" \").replace(\"\\t\", \" \").split()\n",
    "            timestamp = float(parts[0])\n",
    "            data.append((timestamp, parts[1:]))\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rgb_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rgb_path = os.path.join(self.base_dir, self.rgb_data[idx][1][0])\n",
    "        depth_path = os.path.join(self.base_dir, self.depth_data[idx][1][0])\n",
    "        pose_vals = list(map(float, self.gt_data[idx][1]))\n",
    "\n",
    "        rgb = Image.open(rgb_path).convert(\"RGB\")\n",
    "        depth = Image.open(depth_path).convert(\"I\")  # 'I' mode for 32-bit integer pixels\n",
    "\n",
    "        if self.transform:\n",
    "            rgb = self.transform(rgb)\n",
    "        if self.depth_transform:\n",
    "            depth = self.depth_transform(depth)\n",
    "            if depth.ndim == 2:  # If shape is [H, W], add channel dimension\n",
    "                depth = depth.unsqueeze(0)\n",
    "\n",
    "        pose = torch.tensor(pose_vals, dtype=torch.float32)\n",
    "\n",
    "        return {\n",
    "            \"rgb\": rgb,\n",
    "            \"depth\": depth,\n",
    "            \"pose\": pose\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c7fa8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TUMDataset(\n",
    "    rgb_file='rgb.txt',\n",
    "    depth_file='depth.txt',\n",
    "    gt_file='groundtruth.txt',\n",
    "    base_dir=DATASET_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3104851",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TUMTimeDataset(Dataset):\n",
    "    def __init__(self, rgb_file, depth_file, transform=None, depth_transform=None, base_dir=\"\"):\n",
    "        self.base_dir = base_dir\n",
    "\n",
    "        # Read raw data for each stream (excluding ground truth)\n",
    "        self.rgb_data_raw = self._read_file(os.path.join(base_dir, rgb_file))\n",
    "        self.depth_data_raw = self._read_file(os.path.join(base_dir, depth_file))\n",
    "\n",
    "        # Create a single list of all events, tagged by type and including filename\n",
    "        self.event_list = []\n",
    "\n",
    "        # Add RGB events\n",
    "        for timestamp, paths in self.rgb_data_raw:\n",
    "            # paths[0] is the relative path, e.g., \"rgb/1305031441.946399.png\"\n",
    "            self.event_list.append((timestamp, 'rgb', paths[0], os.path.basename(paths[0])))\n",
    "\n",
    "        # Add Depth events\n",
    "        for timestamp, paths in self.depth_data_raw:\n",
    "            # paths[0] is the relative path, e.g., \"depth/1305031441.949704.png\"\n",
    "            self.event_list.append((timestamp, 'depth', paths[0], os.path.basename(paths[0])))\n",
    "\n",
    "        # Sort the entire list of events by timestamp\n",
    "        self.event_list.sort(key=lambda x: x[0])\n",
    "\n",
    "        # Find the minimum starting time and normalize all timestamps\n",
    "        if self.event_list:\n",
    "            min_overall_timestamp = self.event_list[0][0]\n",
    "            self.event_list = [\n",
    "                (timestamp - min_overall_timestamp, event_type, relative_path, filename)\n",
    "                for timestamp, event_type, relative_path, filename in self.event_list\n",
    "            ]\n",
    "        else:\n",
    "            min_overall_timestamp = 0.0 # No data\n",
    "            print(\"Warning: No RGB or Depth data found in the files.\")\n",
    "\n",
    "\n",
    "        self.transform = transform\n",
    "        self.depth_transform = depth_transform\n",
    "        print(f\"Dataset loaded. Total {len(self.event_list)} chronological events (RGB and Depth only).\")\n",
    "        print(f\"Timestamps normalized. First event at time 0.0.\")\n",
    "\n",
    "\n",
    "    def _read_file(self, filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = [line.strip() for line in f if line.strip() and not line.startswith(\"#\")]\n",
    "        data = []\n",
    "        for line in lines:\n",
    "            parts = line.replace(\",\", \" \").replace(\"\\t\", \" \").split()\n",
    "            timestamp = float(parts[0])\n",
    "            # For RGB/Depth, parts[1:] will be just the path.\n",
    "            data.append((timestamp, parts[1:]))\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.event_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Unpack event_data to get relative_path and filename\n",
    "        normalized_timestamp, event_type, relative_path, filename = self.event_list[idx]\n",
    "\n",
    "        result = {\n",
    "            \"timestamp\": normalized_timestamp,\n",
    "            \"event_type\": event_type,\n",
    "            \"filename\": filename # Include the filename in the result\n",
    "        }\n",
    "\n",
    "        if event_type == 'rgb':\n",
    "            rgb_full_path = os.path.join(self.base_dir, relative_path)\n",
    "            rgb = Image.open(rgb_full_path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                rgb = self.transform(rgb)\n",
    "            result[\"rgb\"] = rgb\n",
    "        elif event_type == 'depth':\n",
    "            depth_full_path = os.path.join(self.base_dir, relative_path)\n",
    "            depth = Image.open(depth_full_path).convert(\"I\")  # 'I' mode for 32-bit integer pixels\n",
    "\n",
    "            if self.depth_transform:\n",
    "                depth = self.depth_transform(depth)\n",
    "                # Ensure depth has a channel dimension if needed (e.g., for PyTorch models)\n",
    "                if isinstance(depth, torch.Tensor) and depth.ndim == 2:\n",
    "                    depth = depth.unsqueeze(0)\n",
    "            result[\"depth\"] = depth\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "40830383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded. Total 5094 chronological events (RGB and Depth only).\n",
      "Timestamps normalized. First event at time 0.0.\n"
     ]
    }
   ],
   "source": [
    "timed_data = TUMTimeDataset(\n",
    "    rgb_file='rgb.txt',\n",
    "    depth_file='depth.txt',\n",
    "    base_dir=DATASET_PATH,\n",
    "    # transform=rgb_transform,\n",
    "    # depth_transform=depth_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4aa76db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_to_T_matrix(pose_vec):\n",
    "    \"\"\"\n",
    "    Converts a pose vector [x, y, z, qx, qy, qz, qw] to a 4x4 homogeneous transformation matrix.\n",
    "    \"\"\"\n",
    "    t = pose_vec[:3]\n",
    "    q = pose_vec[3:]\n",
    "    rot_matrix = R.from_quat(q).as_matrix() # scipy.spatial.transform.Rotation handles [x,y,z,w] or [w,x,y,z] based on input\n",
    "    T = np.eye(4)\n",
    "    T[:3, :3] = rot_matrix\n",
    "    T[:3, 3] = t\n",
    "    return T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f7c1651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def T_matrix_to_pose(T_matrix):\n",
    "    \"\"\"\n",
    "    Converts a 4x4 homogeneous transformation matrix to a pose vector [x, y, z, qx, qy, qz, qw].\n",
    "    \"\"\"\n",
    "    t = T_matrix[:3, 3]\n",
    "    rot_matrix = T_matrix[:3, :3]\n",
    "    q = R.from_matrix(rot_matrix).as_quat() # Returns [x,y,z,w]\n",
    "    return np.concatenate((t, q))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d0bff7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unproject_2d_to_3d_depth(u, v, depth_value, K_inv):\n",
    "    \"\"\"\n",
    "    Unprojects a 2D pixel (u,v) with a depth value to a 3D point in camera coordinates.\n",
    "    K_inv is the inverse of the camera intrinsic matrix.\n",
    "    \"\"\"\n",
    "    if depth_value <= 0: # Invalid depth\n",
    "        return None\n",
    "    # Normalized image coordinates\n",
    "    x_c = (u - K[0,2]) / K[0,0] # (u - cx) / fx\n",
    "    y_c = (v - K[1,2]) / K[1,1] # (v - cy) / fy\n",
    "    # 3D point in camera coordinates\n",
    "    P_c = np.array([x_c * depth_value, y_c * depth_value, depth_value])\n",
    "    return P_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "10243311",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def project_3d_to_2d(P_3d_cam, K):\n",
    "    \"\"\"\n",
    "    Projects a 3D point (P_3d_cam) in camera coordinates to 2D pixel coordinates.\n",
    "    Returns (u, v).\n",
    "    \"\"\"\n",
    "    if P_3d_cam[2] <= 0: # Point behind camera or at infinity\n",
    "        return None\n",
    "    u = K[0,0] * (P_3d_cam[0] / P_3d_cam[2]) + K[0,2]\n",
    "    v = K[1,1] * (P_3d_cam[1] / P_3d_cam[2]) + K[1,2]\n",
    "    return u, v\n",
    "\n",
    "# --- 4. EKF Core Functions (Placeholders for complex math) ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "38cca0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_state(current_pose, current_covariance, delta_t, Q_process_noise):\n",
    "    \"\"\"\n",
    "    Predicts the next state and covariance based on a motion model.\n",
    "    This is a simplified constant velocity model. For real EKF, needs Jacobians.\n",
    "    State: [x, y, z, qx, qy, qz, qw]\n",
    "    \"\"\"\n",
    "    # Simple prediction: assume no change in pose, but uncertainty grows\n",
    "    # In a real EKF:\n",
    "    # 1. Augment state with velocity: [x,y,z,qx,qy,qz,qw,vx,vy,vz,wx,wy,wz]\n",
    "    # 2. Propagate x based on velocity and delta_t:\n",
    "    #    predicted_x = f(current_pose, current_velocity, delta_t)\n",
    "    # 3. Propagate P using Jacobian F: predicted_P = F @ current_P @ F.T + Q\n",
    "    # This example only models growing uncertainty.\n",
    "    predicted_pose = current_pose.copy()\n",
    "\n",
    "    # Add process noise (Q_process_noise) to the covariance\n",
    "    # For a proper EKF, Q depends on delta_t and actual noise parameters.\n",
    "    predicted_covariance = current_covariance + Q_process_noise * delta_t # Simplistic noise growth\n",
    "\n",
    "    return predicted_pose, predicted_covariance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "47bb17f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_state_ekf(predicted_state, predicted_covariance, measurement, H_jacobian, R_measurement_noise):\n",
    "    \"\"\"\n",
    "    Performs the Extended Kalman Filter update step.\n",
    "    predicted_state: State vector before measurement (x_k^-)\n",
    "    predicted_covariance: Covariance matrix before measurement (P_k^-)\n",
    "    measurement: The actual sensor measurement (z_k)\n",
    "    H_jacobian: Measurement Jacobian (H_k) - relates state to measurement (h(x))\n",
    "    R_measurement_noise: Covariance of the measurement noise (R_k)\n",
    "    \"\"\"\n",
    "    # Calculate Kalman Gain (K_k)\n",
    "    # y = z_k - h(x_k^-)  # Residual\n",
    "    # S = H_k @ P_k^- @ H_k.T + R_k\n",
    "    # K_k = P_k^- @ H_k.T @ np.linalg.inv(S)\n",
    "\n",
    "    # Update state and covariance\n",
    "    # updated_x = x_k^- + K_k @ y\n",
    "    # updated_P = (I - K_k @ H_k) @ P_k^-\n",
    "\n",
    "    # Placeholder: In a real implementation, this is where the core EKF math happens.\n",
    "    # For now, just return the predicted state as updated.\n",
    "    print(\"  (Placeholder: EKF update logic not implemented here)\")\n",
    "    return predicted_state, predicted_covariance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fab8efd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SLAM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
